X-Mozilla-Status: 0011
X-Mozilla-Status2: 00000000
X-Mozilla-Keys:                                                                                 
Received: by 10.220.49.18 with SMTP id t18csp243900vcf;
        Mon, 29 Oct 2012 12:59:40 -0700 (PDT)
Received: by 10.59.13.135 with SMTP id ey7mr36568390ved.37.1351540774623;
        Mon, 29 Oct 2012 12:59:34 -0700 (PDT)
Received: by 10.230.7.212 with POP3 id e20mf5535414vbe.9;
        Mon, 29 Oct 2012 12:59:34 -0700 (PDT)
Received: from mail-ie0-f181.google.com (mail-ie0-f181.google.com [209.85.223.181])
	(using TLSv1 with cipher RC4-SHA (128/128 bits))
	(No client certificate requested)
	by homiemail-mx21.g.dreamhost.com (Postfix) with ESMTPS id 06A9B6F4474
	for <feeney@gfeeney.com>; Mon, 29 Oct 2012 12:28:34 -0700 (PDT)
Received: by mail-ie0-f181.google.com with SMTP id 16so7233758iea.40
        for <feeney@gfeeney.com>; Mon, 29 Oct 2012 12:28:34 -0700 (PDT)
Received: by 10.50.33.174 with SMTP id s14mr10620647igi.11.1351538914366; Mon,
 29 Oct 2012 12:28:34 -0700 (PDT)
Received: by 10.64.165.193 with HTTP; Mon, 29 Oct 2012 12:28:34 -0700 (PDT)
Return-Path: <tlumley@uw.edu>
Subject: Re: Cross-tabulation of weighted data for very large data sets
From: "Thomas Lumley" <tlumley@uw.edu>
Date: Mon, 29 Oct 2012 15:28:34 -0400
To: "Griffith Feeney" <feeney@gfeeney.com>
Message-ID: <CAJ55+dJcuqZ56Bv5XknMDUU7P4a7rcx7_Zi5VBq5x8orJ1Y69A@mail.gmail.com>
In-Reply-To: <CAK2XK1JtBKLNgiwVQU4iHh=Bvp6UDuPyMz_iYtjv4vCMCY6YFQ@mail.gmail.com>
References: <CAK2XK1JtBKLNgiwVQU4iHh=Bvp6UDuPyMz_iYtjv4vCMCY6YFQ@mail.gmail.com>
MIME-Version: 1.0
Content-Type: text/plain; charset=ISO-8859-1
Delivered-To: griffithfeeney@gmail.com
Received-SPF: neutral (google.com: 209.85.223.181 is neither permitted nor denied by domain of tlumley@uw.edu) client-ip=209.85.223.181;
X-Gmail-Fetch-Info: feeney@gfeeney.com 4 mail.gfeeney.com 110 feeney@gfeeney.com
X-Original-To: feeney@gfeeney.com
Delivered-To: x11083680@homiemail-mx21.g.dreamhost.com
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=google.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type:x-gm-message-state;
        bh=cQ0E061Em128rpqy2+NtasdJiA8RWm7VR/ARl1fp6wQ=;
        b=XbuZMRasLjMyYD0s3zipjoMN+rKeZ2TgqgFEGhp1Hd3WO63X37ygMripN7wzW1frR1
         iaETFmZNwjPkcxl07i2F/LucUEXmV29wARjulXfYliD6VFEed9topkg7edk3SPjMEJsv
         MM9E/mP58tvNxpiZMnzuDtZoNb6VvQliDHsoIgRffvxLT9PZXMRNJZqLN7L1zImarKSJ
         CmY+9M4LIdbU+ObhE32J2bkmugqYEgpariIMXliBu7ImoqQlaydZPXjG0CS0n3n340/y
         7MNMjwfg28CB6RJLPEj6ADQn2O7BThq0dAvLLU6f5/3hOjwbve0sizBS08mXszPHWXim
         xcYA==
X-Gm-Message-State: ALoCoQl30IM4ftLOB6/mbsA6WfWFKz1EWDlZIYipAFcIcUq0uUnX0Z9DHBbub1EbT54jG9O1SmDv
X-Converted-By: Emailchemy 12.0 Personal Edition; licensedTo="Griffith Feeney"

Griffith,

The rowsum() function would compute the table entries rapidly. They
won't be in contingency-table layout, but that's relatively easily
fixed.

For example, with a ten million row data frame because my desktop
isn't all that powerful

df<-data.frame(x=sample(10,1e6,replace=TRUE),y=rep(1:2,5e6),w=rexp(1e7))
> system.time(a<-rowsum(df$w,paste(df$x,df$y)))
   user  system elapsed
  6.138   0.177   6.288
> a
         [,1]
1 1  500034.3
1 2  502368.5
10 1 497532.2
10 2 500391.8
2 1  501079.3
2 2  498638.0
3 1  498285.0
3 2  497506.2
4 1  502648.7
4 2  499396.5
5 1  501908.3
5 2  500862.1
6 1  502862.0
6 2  502347.4
7 1  500956.1
7 2  500141.3
8 1  501769.8
8 2  497672.0
9 1  498840.9
9 2  499760.5

That's about 5% slower than just table(df$x,df$y).


I also might try doing this in SQL: if the data are in a database
table called 'data', then

   select age, sex,sum(wt) from data group by age,sex order by age,sex

That's what I've been doing with SQL generated by R in a new package
for analysing very large surveys (such as ACS or the Nationwide
Inpatient Sample): http://sqlsurvey.r-forge.r-project.org/

The SQL approach probably won't be any faster if your computer has
enough memory, but it can be much better when the data don't fit in
physical memory.


   -thomas


On Mon, Oct 29, 2012 at 6:08 PM, Griffith Feeney <feeney@gfeeney.com> wrote:
> Hello Thomas,
>
> Got your contact from the 'survey' package documentation.
>
> I've been experimenting with using R for tabulating census data--relatively
> few variables, large data sets, e.g., 44 million records.
>
> From the source flat file for persons I create, e.g., two text files, one
> for age, one for sex, each containing 44 million lines. The age file has 3
> character lines, the sex file 1 character lines. readLines() gets these into
> R and table(age,sex) produces a table, both remarkably rapidly. 10-20 times
> faster than the US Census Bureau's CSPro package (the old IMPS is much
> faster than the newer CSPro, but a nuisance to use).
>
> But I need a table() function that will take weights as an argument, e.g.,
> table(age,sex,weights=weights.vector), where weights.vector is a numeric
> vector of the same length as age and sex. Weights come from a
> Post-Enumeration Survey.
>
> I've spent several hours searching, but what I've found seems either (a)
> much more complicated than what I need--cross-tabulations only--and/or (b)
> perhaps not suitable for very large data sets. I want to avoid trial and
> error if possible.
>
> Ideas, pointers to possible solutions?
>
> Many thanks!
>
> Griffith Feeney
>
> --
> Griffith Feeney Ph.D.
> Scarsdale, New York
> Demography - Statistics - IT
> South Africa Mobile +27 84 238 1991
> Land +1 914 725 2218
> Mobile A +1 914 364 6156
> Mobile B +1 914 707 2673
> Skype gfeeney
> Email feeney@demographer.com
> Web demographer.com - gfeeney.com
>



-- 
Thomas Lumley
Professor of Biostatistics
University of Auckland

